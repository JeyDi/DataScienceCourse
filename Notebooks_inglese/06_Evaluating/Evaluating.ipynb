{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inizio'></a>\n",
    "# Evaluating\n",
    "\n",
    "In this notebook we'll present you the mainly topics about evaluating performance and structurized the machine learning processes.\n",
    "<br><br>\n",
    "This notebook will present the following topics:\n",
    "- [Choosing the Right Estimator](#right_estimator)<a href='#right_estimator'></a> <br>\n",
    "- [Confusion Matrix](#conf_matrix)<a href='#conf_matrix'></a> <br>\n",
    "- [Scoring](#scoring)<a href='#scoring'></a>\n",
    "- [Cross-Validation](#cross_validation)<a href='#cross_validation'></a>\n",
    "- [Power-Tuning](#power_tuning)<a href='#power_tuning'></a>\n",
    "- [Pipeling](#pipeling)<a href='#pipeling'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='right_estimator'></a>\n",
    "### Choosing the right estimator\n",
    "Typically, algorithm choice is dictated by a balance of factors: <br>\n",
    "- The dimensionality of your data; <br>\n",
    "- The geometric nature of your data; <br>\n",
    "- The types of features used to represent your data; <br>\n",
    "- The number of training samples you have at your disposal; <br>\n",
    "- The required training and prediction speeds needed for your purposes; <br>\n",
    "- The predictive accuracy level desired; <br>\n",
    "- How configurable you need your model to be; <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following flow-chart describes the circumstances under which you should use the different machine learning algorithms:\n",
    "<img src='Algorithm Cheat Sheet.jpg'>\n",
    "<img src='ScikitLearn_Map.png'>\n",
    "<br>\n",
    "[Click here to return to the top of the pag](#inizio)<a href='#inizio'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conf_matrix'></a>\n",
    "### Confusion Matrix\n",
    "A universal method you can use to deeply study how well any of your supervised learning predictors is doing is by using a **confusion matrix**. <br>\n",
    "A confusion matrix displays your model's predicted (**testing set**) outputs against the true observational values.<br>\n",
    "This helps you see how well your algorithm was able to generalize and identify specific target labels, along with which labels were often\n",
    "confused. <br>\n",
    "This can be helpful in increasing your accuracy, because you can then engineer additional features that help to better identify highly confusing targets, and then take another run through your data analysis pipeline. <br>\n",
    "Traditionally, the predicted targets are aligned on the X-axis of the matrix, and the true values are aligned on the Y-axis. Let's say you have the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "y_true = [1,1,2,2,3,3] # Actual, observed testing dataset values\n",
    "y_pred = [1,1,1,3,2,3] # Predicted values from your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true labels are encoded data representing cats, dogs, and monkeys, for the three values. <br>\n",
    "You can compute a confusion matrix using SciKit-Learn as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a clearer representation of the data would look like this:\n",
    "<img src='example.jpg'>\n",
    "<br>\n",
    "You can derive quite a bit of information from your confusion matrix. <br>\n",
    "The first is how many actual cats, dogs, and monkeys you have. By summing up the values in a row, you'll know the true count of your data. <br>\n",
    "You can do similarly with the columns, to see how many times your model predicted a certain target. <br>\n",
    "By adding up all the values in the Predicted Dog column, we can see our model thought our testing dataset only had a single dog in it. <br>\n",
    "An important thing to realize is that all of the non-diagonal elements\n",
    "of the matrix correspond to misclassified targets.<br>\n",
    "Given all this information, you're able to derive probabilities relating to how accurate your answers are. \n",
    "<br>\n",
    "<br>\n",
    "Given the example above, your algorithm predicted there were two cats in the dataset, and there indeed were two cats. In fact, the two samples the algorithm believed to be cats turned out to be the actual cats. It looks like the model is very good at identifying cats.\n",
    "<br>\n",
    "<br>\n",
    "On the other hand, there were two dogs in the dataset. The model somehow came to the conclusion that one of the monkey's was a dog, and din't even arrive at two dog predictions. It looks like you trained a non-dog friendly model. Regarding monkeys, there were two in the dataset. One of them, the algorithm predicted correctly. The other, the model thought was a dog. It looks like there is some level of confusion here between monkeys and dogs. This is a good indicator that you might consider adding additional feature to your dataset, such as banana-affinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "columns = ['Cat', 'Dog', 'Monkey']\n",
    "confusion = metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.imshow(confusion, cmap=plt.cm.Blues, interpolation='nearest')\n",
    "plt.xticks([0,1,2], columns, rotation='vertical')\n",
    "plt.yticks([0,1,2], columns)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here to return to the top of the pag](#inizio)<a href='#inizio'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scoring'></a>\n",
    "### Scoring\n",
    "There are some concepts that come into play when evaluating the machine learning models. <br>\n",
    "They are different as we use classification models, regression, clustering and so on.<br>\n",
    "Starting with classification models is important understand that when a model try to predict a label of a dataset it is possible to identify the prediction as:\n",
    "- **True positive**: when the item is correctly labeled as belonging to the positive class;\n",
    "- **True negative**: when the item is correctly labeled as belonging to the negative class;\n",
    "- **False positive**: when item is incorrectly labeled as belonging to the positive class;\n",
    "- **False negative**: when item isn't labeled as belonging to the positive class but should have been."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classification task, the **precision** (also called _**positive predictive value**_) _for a class is the number of true positives divided by the total number of elements labeled as belonging to the positive class_ (i.e. the sum of true positives and false positives).<br>\n",
    "**Recall** (also called _**sensitivity**_) _is defined as the number of true positives divided by the total number of elements that actually belong to the positive class_ (i.e. the sum of true positives and false negatives).<br>\n",
    "<img src='Precisionrecall.svg'> <br>\n",
    "A *precision* score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a *recall* of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C). <br>\n",
    "\n",
    "The terms *positive* and *negative* refer to the classifier's prediction (sometimes known as the expectation), and the terms *true* and *false* refer to whether that prediction corresponds to the external judgment (sometimes known as the observation). <br>\n",
    "Considering the *confusion matrix* of the earlier paragraph's example, and focus on the cat class. We can build this *table of confusion*: <br>\n",
    "<img src='Table.jpg'>\n",
    "With this table completed we can perform other metrics as **true negative rate** and **accuracy**:\n",
    "<img src='accuracy.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKit-Learn's metrics model helps you calculate many of these metrics automatically. <br>\n",
    "Given the following setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "y_true = [1,1,1,2,2,2] # Actual, observed testing dataset values\n",
    "y_pred = [1,1,2,2,2,2] # Predicted values from your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1],\n",
       "       [0, 3]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.67\n",
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", metrics.precision_score(y_true, y_pred))\n",
    "print(\"Recall:\", round(metrics.recall_score(y_true, y_pred),2))\n",
    "print(\"Accuracy:\", round(metrics.accuracy_score(y_true, y_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another metric computing for classification model is **ROC Curve** (_receveir operating characteristic curve_), that is a graph showing the performance at all classification thresholds. The curve plots 2 parameters:<br>\n",
    "- True Positive Rate;\n",
    "- False Positive Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True Positive Rate** (TPR) is a synonym for recall and is therefore defined as follows: <br>\n",
    "<img src='TPR.jpg'>\n",
    "**False Positive Rate** (FPR) is defined as follows:\n",
    "<img src='FPR.jpg'>\n",
    "The following figure shows a typical ROC curve.\n",
    "<img src='ROC.jpg'>\n",
    "To compute the points in an ROC curve, we could evaluate a logistic regression model many times with different classification thresholds, but this would be inefficient. Fortunately, there's an efficient, sorting-based algorithm that can provide this information for us, called AUC. <br>\n",
    "**AUC** stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).\n",
    "<img src='AUC.jpg'>\n",
    "One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. <br>\n",
    "_AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0_.<br>\n",
    "To investigate other scoring methods for other machine learning algorithms look the following links:\n",
    "- [Regression metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n",
    "- [Clustering metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#clustering-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here to return to the top of the pag](#inizio)<a href='#inizio'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cross_validation'></a>\n",
    "### Cross Validation\n",
    "We're already well aware of the importance of splitting our data into training and testing sets to validate the accuracy of our models by checking how well it fits the data. This method of avoiding overfitting introduces three issues:\n",
    "- Each time we run train/test split, unless we set the random_state parameter, we're going to get back different accuracy scores; \n",
    "- The second issue introduced is by withholding data from training, we essentially lose some of our training data. Machine learning is only as accurate as the data its trained upon, so generally more data means better results. Neglecting to train our models on our hard collected data is like refusing to take our rightful change at the bank;\n",
    "- But the most important issue introduced is that with some of the more configurable estimators, such as SVC, we will probably end up running our model many times while tinkering with the various parameters, such as C and gamma for producing optimal results. By doing this, we will leak some information from our testing set into our training set. Our model, armed with these secret details about our testing set, might still perform poorly in the real-world if it overfit that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to overcome this is by using **cross_val_score()** method. <br>\n",
    "This method takes as input our model along with our training dataset and performs K-fold cross validations on it. <br>\n",
    "In other words, our training data is first cut into a number of 'K' sets. Then, \"K\" versions of our model are trained, each using an independent K-1 number of the \"K\" available sets. Each model is evaluated with the last set, it's **out-of-bag set**. <br>\n",
    "Here is the code for cross validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Cross Validation on your training data\n",
    "from sklearn import cross_validation as cval\n",
    "\n",
    "cval.cross_val_score(model, X_train, y_train, cv=10)\n",
    "\n",
    "cval.cross_val_score(model, X_train, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation allows us to use all the data we provide as both training and testing. <br>\n",
    "Many resources online will recommend us don't even do the extra step of splitting our data into a training and testing set and just feed the lot directly into our cross validator. There are advantages and disadvantages of this:\n",
    "- The main advantage is the overall simplicity of your process;\n",
    "- The disadvantage is that it still is possible for some information to leak into your training dataset, as we discussed above with the SVC example. This information leak might even occur prior to us fitting our model, for example it might be at the point of transforming our data using isomap or principle component analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the wild, the best process to use depending on how many samples we have at our disposal and the machine learning algorithms we are using, is either of the following: <br>\n",
    "1) Split our data into training, validation, and testing sets; <br>\n",
    "2) Setup a pipeline, and fit it with our **training** set; <br>\n",
    "3) Access the accuracy of its output using our **validation** set; <br>\n",
    "4) Fine tune this accuracy by adjusting the hyper-parameters of our pipeline; <br>\n",
    "5) When we are comfortable with its accuracy, finally evaluate our pipeline with the **testing** set. <br>\n",
    "<br>\n",
    "OR <br>\n",
    "<br>\n",
    "1) Split our data into training and testing sets; <br>\n",
    "2) Setup a pipeline with CV and fit/score it with our **training** set; <br>\n",
    "3) Fine tune this accuracy by adjusting the hyper-parameters of our pipeline; <br>\n",
    "4) When we are comfortable with its accuracy, finally evaluate our pipeline with the **testing** set. <br>\n",
    "<br>\n",
    "<br>\n",
    "Useful link:\n",
    "- [Cross-Validation iterators](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here to return to the top of the pag](#inizio)<a href='#inizio'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='power_tuning'></a>\n",
    "### Power Tuning\n",
    "The method used for parameter tuning is **GridSearchCV**. <br>\n",
    "In its simplest form, GridSearchCV works by taking in an estimator, a grid of parameters you want optimized, and your cv split value. This is the example from [SciKit-Learn's API page](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn import svm, datasets\n",
    ">>> from sklearn.model_selection import GridSearchCV\n",
    ">>> iris = datasets.load_iris()\n",
    ">>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 5, 10]}\n",
    ">>> svc = svm.SVC(gamma=\"scale\")\n",
    ">>> clf = GridSearchCV(svc, parameters, cv=3)\n",
    ">>> clf.fit(iris.data, iris.target)\n",
    "...                             \n",
    "GridSearchCV(cv=3, error_score=...,\n",
    "       estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,\n",
    "                     decision_function_shape='ovr', degree=..., gamma=...,\n",
    "                     kernel='rbf', max_iter=-1, probability=False,\n",
    "                     random_state=None, shrinking=True, tol=...,\n",
    "                     verbose=False),\n",
    "       fit_params=None, iid=..., n_jobs=None,\n",
    "       param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,\n",
    "       scoring=..., verbose=...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, GridSearchCV is being used to optimize a support vector classifier model. <br>\n",
    "Since the exact parameters have been specified, GridSearchCV will build a table of every combination (but not permutation) of the available parameters and crossvalidate each one separately: <br>\n",
    "<img src='power_tuning.jpg'> <br>\n",
    "In addition to explicitly defining the parameters you want tested, you can also use randomized parameter optimization with SciKit-Learn's [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) class. The semantics are a bit different here.<br> First, instead of passing a list of grid objects (with GridSearchCV, you can actually perform multiple grid optimizations, consecutively), this\n",
    "time you pass in a your parameters as a single dictionary that holds either possible, discrete parameter values or distribution over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dist = {\n",
    "'C': scipy.stats.expon(scale=100),\n",
    "'kernel': ['linear'],\n",
    "'gamma': scipy.stats.expon(scale=.1),\n",
    "}\n",
    "\n",
    "classifier = grid_search.RandomizedSearchCV(model, parameter_dist)\n",
    "classifier.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomizedSearchCV also takes in an optional n_iter parameter you can use to control the number of parameter settings that are sampled. Regardless of the cross validation search tool you end up using, after all of the methods exposed by the class are ran using the estimator that maximized the score of the out-of-bag data. So in the examples above, the .fit() method along with any subsequent methods, such as .predict(), .score(), .transform(), .predict() are all executed and return values as-if they were called on the best found estimator directly.<br>\n",
    "SciKit-Learn has [a very nice example](https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py) that compares the execution times as well as scoring results of randomized search versus grid search, while trying to optimize various random forest\n",
    "parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here to return to the top of the pag](#inizio)<a href='#inizio'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipeling'></a>\n",
    "### Pipeling\n",
    "SciKit-Learn has created a pipelining class, it wraps around your entire data analysis pipeline from start to finish, and allows you to interact with the pipeline as if it were a single white-box, configurable estimator. <br>\n",
    "The other added benefit is that once your pipeline has been built, since the pipeline inherits from the estimator base class, you can use it pretty much anywhere you'd use regular estimators-including in your cross validator method. Doing so, you can simultaneously fine tune the parameters of each of the estimators and predictors that comprise your data-analysis pipeline. <br>\n",
    "If you don't want to encounter errors, there are a few rules you must abide by while using SciKit-Learn's pipeline: <br>\n",
    "- Every intermediary model, or step within the pipeline must be a transformer. That means its class must implement both the .fit() and the .transform() methods. This is rather important, as the output from each step will serve as the input to the subsequent step;\n",
    "- The very last step in your analysis pipeline only needs to implement the .fit() method, since it will not be feeding data into another step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to get up and running with your own pipelines looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "svc = svm.SVC(kernel='linear')\n",
    "pca = RandomizedPCA()\n",
    "pipeline = Pipeline([('pca', pca), ('svc', svc)])\n",
    "pipeline.set_params(pca__n_components=5, svc__C=1, svc__gamma=0.0001)\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when you define parameters, you have to lead with the name you specified for that parameter when you added it to your pipeline, followed by two underscores and the parameter name. This is important because there are many estimators that share the same parameter names within SciKit-Learn's API. Without this, there would be ambiguity. <br>\n",
    "The pipeline class only has a single attribute called .named_steps, which is a dictionary containing the estimator names you specified as keys. <br>\n",
    "You can use it to gain access to the underlying estimator steps within your pipeline. Besides directly specifying estimators, you can also have feature unions and nested pipelines as well! On top of that, you can implement your own custom transformers as a minimal class, so long as you provide end-points for .fit(), and .transform()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some useful links:**\n",
    "- [Emanuel Ferm - Cheat Sheet](http://eferm.com/machine-learning-cheat-sheet/)\n",
    "- [Estimator Parameter Search-Space](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "- [Getting Crazy With Pipelining](http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html)\n",
    "- [Overfitting](https://en.wikipedia.org/wiki/Overfitting)\n",
    "\n",
    "With this paragraph ends the notebook \"Evaluating\" and the Python Course.\n",
    "<br><br>\n",
    "- [Click here to return to the top of the pag](#inizio)<a href='#inizio'></a>\n",
    "<br><br>\n",
    "If you have any doubts, you can write to us on Teams!<br>\n",
    "See you soon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
